{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision.models import resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성 추출기 layer\n",
    "\n",
    "# vgg_config = [64, 'N', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "def get_vgg_layer(config, batch_norm):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "\n",
    "    for c in config:\n",
    "        assert c == 'M' or isinstance(c, int)\n",
    "        \n",
    "        if c == 'M': # c가 M이면 MaxPooling\n",
    "            layers += [nn.MaxPool2d(kernel_size= 2)]\n",
    "\n",
    "        else: # c가 int면 Convolution\n",
    "            conv2d = nn.Conv2d(in_channels, c, kernel_size= 3, padding= 1)\n",
    "\n",
    "            # batch normalization 적용\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace= True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace= True)]\n",
    "\n",
    "            in_channels = c # 다음 layer의 in_channels로 사용\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    # attribute -> enc_hidden_size, dec_hidden_size, dec_num_layer, pixel_size, resnet, pooling, relu\n",
    "    # method -> relu, hidden_dim_changer\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_hidden_size = config.enc_hidden_size   # encoding된 이미지 feature의 width, height\n",
    "        self.dec_hidden_size = config.dec_hidden_size   # LSTM decoder의 hidden dimension\n",
    "        self.dec_num_layers = config.dec_num_layers     # LSTM decoder의 레이어 수\n",
    "        self.pixel_size = self.enc_hidden_size * self.enc_hidden_size   # encoding된 이미지 feature의 전체 픽셀수 -> width * height\n",
    "\n",
    "        base_model = resnet101(pretrained= True, progress= False)   # Encoder의 backbone model은 resnet101\n",
    "        base_model = list(base_model.children())[:-2]               # 마지막 2개의 layer는 제거 (추론기)\n",
    "        self.resnet = nn.Sequential(*base_model)                    # resnet은 Sequential 객체\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((self.enc_hidden_size, self.enc_hidden_size))   # encoding된 feature의 가로, 세로 크기를 맞춰주기 위한 pooling layer\n",
    "        # output_size = enc_hidden_size * enc_hidden_size\n",
    "\n",
    "        self.relu = nn.ReLU() # 렐루는렐루~\n",
    "        self.hidden_dim_changer = nn.Sequential(\n",
    "            nn.Linear(in_features= self.pixel_size,\n",
    "                      out_features= self.dec_hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Encoding된 이미지의 feature를 LSTM decoder의 hidden, cell state로 만들기 위한 레이어\n",
    "        self.h_mlp = nn.Linear(in_features= 2048,\n",
    "                               out_features= self.dec_hidden_size)\n",
    "        self.c_mlp = nn.Linear(in_features= 2048,\n",
    "                               out_features= self.dec_hidden_size)\n",
    "\n",
    "        self.fine_tune(True)\n",
    "\n",
    "    # fine tuning 하는 함수\n",
    "    def fine_tune(self, fine_tune= True):\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # pretrained 모델의 첫 5개를 제외한 레이어만 학습\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune\n",
    "\n",
    "    # 이미지 학습\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = self.resnet(x)\n",
    "        x = self.pooling(x)\n",
    "        x = x.view(batch_size, 2048, -1)\n",
    "\n",
    "        # decoder의 layer수가 1이 아니 경우 -> hidden, cell state의 차원을 맞춤\n",
    "        if self.dec_num_layers != 1:\n",
    "            tmp = self.hidden_dim_changer(self.relu(x))\n",
    "        else:\n",
    "            tmp = torch.mean(x, dim= 2, keepdim= True)\n",
    "        tmp = torch.permute(tmp, (2, 0, 1))\n",
    "        h0 = self.h_mlp(tmp)\n",
    "        c0 = self.c_mlp(tmp)\n",
    "\n",
    "        return x, (h0, c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config, tokenizer):\n",
    "        super(Decoder, self).__init__()\n",
    "        # attribute -> pixel_size, dec_hidden_size, dec_num_layers, dropout, is__attn,\n",
    "        #              pad_token_id, vocab_size, input_size\n",
    "        # method -> \n",
    "        self.pixel_size = config.enc_hidden_size * config.enc_hidden_size\n",
    "        self.dec_hidden_size = config.dec_hidden_size\n",
    "        self.dec_num_layers = config.dec_num_layers\n",
    "\n",
    "        self.dropout = config.dropout\n",
    "        self.is_attn = config.is_attn # Attention layer인지 여부\n",
    "\n",
    "        self.pad_token_id = tokenizer.pad_token_id  # 토크나이저의 pad token id\n",
    "        self.vocab_size = tokenizer.vocab_size      # 토크나이저의 vocab size\n",
    "\n",
    "        # attention layer\n",
    "        if self.is_attn:\n",
    "            self.attention = Attention(self.dec_hidden_size)        \n",
    "        # input_size는 attention layer인지 decoder인지에 따라 변경\n",
    "        # attention을 사용하는 경우 decoder input의 차원은 2048(=Attention 결과) 증가\n",
    "        self.input_size = self.dec_hidden_size + 2048 if self.is_attn else self.dec_hidden_size\n",
    "\n",
    "        # 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.dec_hidden_size, padding_idx= self.pad_token_id)\n",
    "        \n",
    "        # lstm 레이어\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
    "                            hidden_size= self.dec_hidden_size,\n",
    "                            num_layers= self.dec_num_layers,\n",
    "                            batch_first= True)\n",
    "        \n",
    "        # dropout 레이어\n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # beta 레이어\n",
    "        self.beta = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.dec_hidden_size, 2048),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # 다음 단어를 예측하는 vocab size의 크기만큼 내어주는 fully connected 레이어\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.dec_hidden_size, self.vocab_size)\n",
    "        )\n",
    "\n",
    "        # init_weights 함수를 통해 모델 내부에 가중치를 적용\n",
    "        self.embedding.apply(self.init_weights)\n",
    "        self.fc.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        # 선형 레이어 -> bias는 0, data는 [-0.1, 0.1] 구간 랜덤 초기화\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.bias.data.fill_(0)\n",
    "            m.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "        # 임베딩 레이어 -> data [-0.1, 0.1] 구간 랜덤 초기화\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            m.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    # target 캡션이 학습시 거치는 부분\n",
    "    def forward(self, x, hidden, enc_output):\n",
    "        x = self.embedding(x)\n",
    "        score = None\n",
    "\n",
    "        gate = self.beta(hidden[0][-1])\n",
    "        if self.is_attn:\n",
    "            enc_output, score = self.attention\n",
    "            enc_output = gate * enc_output\n",
    "            x = torch.cat((x, enc_output.unsqueeze(1)), dim= -1)\n",
    "            x, hidden = self.lstm(x, hidden)\n",
    "            x = self.fc(x)\n",
    "\n",
    "        return x, hidden, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention\n",
    "# Attention 모듈에 encoder의 output과 decoder의 이전 output 결과가 들어감\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        # attribute -> hidden_size, enc_wts, dec_wts, score_wts\n",
    "        # method -> tanh, relu\n",
    "\n",
    "        # hidden_size는 입력\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # encoder weights\n",
    "        self.enc_wts = nn.Sequential(\n",
    "            nn.Linear(2048, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        )\n",
    "        # decoder weights\n",
    "        self.dec_wts = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        )\n",
    "\n",
    "        # score weights\n",
    "        self.score_wts = nn.Linear(self.hidden_size, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # 순전파\n",
    "    def forward(self, enc_output, dec_hidden): # encoder의 output과 decoder의 hidden_state를 입력\n",
    "        # encoder output을 차원변환\n",
    "        enc_output = torch.permute(enc_output, (0, 2, 1))\n",
    "\n",
    "        # score는 \n",
    "        score = self.tanh(self.enc_wts(enc_output) + self.dec_wts(dec_hidden).unsqueeze(1))\n",
    "        score = self.score_wts(score)\n",
    "        score = F.softmax(score, dimn= 1)\n",
    "\n",
    "        enc_output = torch.permute(enc_output, (0, 2, 1))       # enc_output 텐서를 차원변경\n",
    "        enc_output = torch.bmm(enc_output, score).squeeze(-1)   # \n",
    "\n",
    "        return enc_output, score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
