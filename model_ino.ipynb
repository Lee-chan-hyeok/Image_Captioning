{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision.models import resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성 추출기 layer\n",
    "\n",
    "# vgg_config = [64, 'N', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "def get_vgg_layer(config, batch_norm):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "\n",
    "    for c in config:\n",
    "        assert c == 'M' or isinstance(c, int)\n",
    "        \n",
    "        if c == 'M': # c가 M이면 MaxPooling\n",
    "            layers += [nn.MaxPool2d(kernel_size= 2)]\n",
    "\n",
    "        else: # c가 int면 Convolution\n",
    "            conv2d = nn.Conv2d(in_channels, c, kernel_size= 3, padding= 1)\n",
    "\n",
    "            # batch normalization 적용\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace= True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace= True)]\n",
    "\n",
    "            in_channels = c # 다음 layer의 in_channels로 사용\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Encoder, self).__init__()\n",
    "        # hidden size, layer 수, pixel 크기\n",
    "        self.enc_hidden_size = config.enc_hidden_size\n",
    "        self.dec_hidden_size = config.dec_hidden_size\n",
    "        self.dec_num_layers = config.dec_num_layers\n",
    "        self.pixel_size = self.enc_hidden_size * self.enc_hidden_size\n",
    "\n",
    "        base_model = resnet101(pretrained= True, progress= False)\n",
    "        # 추론기 부분은 제거\n",
    "        base_model = list(base_model.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*base_model)\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((self.enc_hidden_size, self.enc_hidden_size))\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_dim_changer = nn.Sequential(\n",
    "            nn.Linear(self.pixel_size, self.dec_hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.h_mlp = nn.Linear(2048, self.dec_hidden_size)\n",
    "        self.c_mlp = nn.Linear(2048, self.dec_hidden_size)\n",
    "\n",
    "        self.fine_tune(True)\n",
    "\n",
    "    def fine_tune(self, fine_tune= True):\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = self.resnet(x)\n",
    "        x = self.pooling(x)\n",
    "        x = x.view(batch_size, 2048, -1)\n",
    "\n",
    "        if self.dec_num_layers != 1:\n",
    "            tmp = self.hidden_dim_changer(self.relu(x))\n",
    "        else:\n",
    "            tmp = torch.mean(x, dim= 2, keepdim= True)\n",
    "        tmp = torch.permute(tmp, (2, 0, 1))\n",
    "        h0 = self.h_mlp(tmp)\n",
    "        c0 = self.c_mlp(tmp)\n",
    "\n",
    "        return x, (h0, c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config, tokenizer):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.pixel_size = config.enc_hidden_size * config.enc_hidden_size\n",
    "        self.dec_hidden_size = config.dec_hidden_size\n",
    "        self.dec_num_layers = config.dec_num_layers\n",
    "\n",
    "        self.dropout = config.dropout\n",
    "        self.is_attn = config.is_attn # Attention layer인지 여부\n",
    "\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "        # attention layer\n",
    "        if self.is_attn:\n",
    "            self.attention = Attention(self.dec_hidden_size)        \n",
    "        # input_size는 attention layer인지 decoder인지에 따라 변경\n",
    "        self.input_size = self.dec_hidden_size + 2048 if self.is_attn else self.dec_hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.dec_hidden_size, padding_idx= self.pad_token_id)\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
    "                            hidden_size= self.dec_hidden_size,\n",
    "                            num_layers= self.dec_num_layers,\n",
    "                            batch_first= True)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.beta = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.dec_hidden_size, 2048),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.dec_hidden_size, self.vocab_size)\n",
    "        )\n",
    "\n",
    "        self.embedding.apply(self.init_weights)\n",
    "        self.fc.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.bias.data.fill_(0)\n",
    "            m.weight.data.uniform_(-0.1, 0.1)\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            m.weight.data.uniform_(-0.1, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
