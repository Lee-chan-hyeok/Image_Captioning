{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        resnet = models.resnet101(pretrained= True)\n",
    "        modelus = list(resnet.children())[:-2] # 마지막 layer 제외\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1, 1)\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum= 0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = self.avgpool(features)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.bn(self.fc(features))\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Decoder\n",
    "class BertDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, num_layers, vocab_size):\n",
    "        super(BertDecoder, self).__init__()\n",
    "        self.bert_config = BertConfig(hidden_size= hidden_size,\n",
    "                                      num_hidden_layers= num_layers,\n",
    "                                      num_attention_heads= 8,\n",
    "                                      intermediate_size= hidden_size)\n",
    "        self.bert = BertModel(self.bert_config)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, encoded_images, captions, lengths):\n",
    "        embeddings = self.bert(captions)[0]\n",
    "        embeddings = pack_padded_sequence(embeddings, lengths, batch_first=True).data\n",
    "        \n",
    "        features = torch.cat([encoded_images.unsqueeze(1), embeddings], dim=1)\n",
    "        outputs = self.fc(features)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 모델 정의 (ResNet + BERT)\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, num_layers, vocab_size):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.encoder = ResNetEncoder(embed_size)\n",
    "        self.decoder = BertDecoder(embed_size, hidden_size, num_layers, vocab_size)\n",
    "\n",
    "    def forward(self, images, captions, lengths):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions, lengths)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 로딩 예시 (MSCOCO 데이터셋을 사용한다고 가정)\n",
    "# 이미지는 사전에 전처리되어야 하며, 캡션은 토크나이징이 되어야 합니다.\n",
    "# 데이터 로더와 학습 루프 등은 데이터셋에 따라 구성해야 합니다.\n",
    "\n",
    "# 모델 초기화\n",
    "embed_size = 512  # 이미지와 문장 임베딩 크기\n",
    "hidden_size = 512  # BERT hidden size\n",
    "num_layers = 6  # BERT layer 수\n",
    "vocab_size = 10000  # 어휘 크기 (예시용)\n",
    "\n",
    "model = ImageCaptioningModel(embed_size, hidden_size, num_layers, vocab_size)\n",
    "\n",
    "# 손실 함수와 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 루프 예시 (실제 데이터셋에 맞게 수정 필요)\n",
    "for epoch in range(num_epochs):\n",
    "    for images, captions, lengths in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, captions, lengths)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 학습 과정 출력 (필요에 따라 추가)\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 학습 완료 후 모델 저장 (필요에 따라 추가)\n",
    "torch.save(model.state_dict(), 'image_captioning_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
