{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MS-COCO 데이터셋 다운로드 및 전처리: 이미지와 캡션 데이터를 준비합니다.\n",
    "- 특징 추출 모델 (VGG): 이미지를 입력받아 특징을 추출합니다.\n",
    "- 캡셔닝 모델 (Transformer): 추출된 특징을 이용해 이미지를 설명하는 문장을 생성합니다.\n",
    "- 여기서는 Python과 PyTorch를 사용하여 VGG 모델로 특징을 추출하고, Transformer 모델을 사용해 캡션을 생성하는 과정을 단계별로 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터셋 다운로드 및 전처리\n",
    "MS-COCO 데이터셋은 매우 크기 때문에 부분만 사용하는 것이 좋습니다. 먼저, 데이터셋을 다운로드하고 전처리하는 코드를 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# 데이터셋 경로 설정\n",
    "data_dir = 'path_to_coco_dataset'\n",
    "train_dir = os.path.join(data_dir, 'train2017')\n",
    "ann_file = os.path.join(data_dir, 'annotations', 'captions_train2017.json')\n",
    "\n",
    "# 이미지 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# COCO 데이터셋 로드\n",
    "coco = COCO(ann_file)\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, root, annFile, transform=None):\n",
    "        self.root = root\n",
    "        self.coco = COCO(annFile)\n",
    "        self.ids = list(self.coco.anns.keys())\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ann_id = self.ids[index]\n",
    "        caption = self.coco.anns[ann_id]['caption']\n",
    "        img_id = self.coco.anns[ann_id]['image_id']\n",
    "        path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "        \n",
    "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, caption\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 인스턴스 생성\n",
    "dataset = CocoDataset(root=train_dir, annFile=ann_file, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. VGG 모델을 이용한 특징 추출\n",
    "VGG 모델을 이용해 이미지에서 특징을 추출하는 코드를 작성합니다. 여기서는 torchvision 라이브러리의 미리 학습된 VGG 모델을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# 미리 학습된 VGG 모델 로드\n",
    "vgg = models.vgg16(pretrained=True)\n",
    "vgg.classifier = torch.nn.Sequential(*list(vgg.classifier.children())[:-1])  # 마지막 FC layer 제거\n",
    "vgg.eval()\n",
    "\n",
    "def extract_features(image):\n",
    "    with torch.no_grad():\n",
    "        features = vgg(image)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Transformer 모델 정의 및 학습\n",
    "이제 Transformer 모델을 정의하고 학습하는 코드를 작성합니다. 여기서는 PyTorch의 nn.Transformer를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, feature_dim, vocab_size, embed_size, num_heads, num_layers):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.feature_embed = nn.Linear(feature_dim, embed_size)\n",
    "        self.transformer = nn.Transformer(d_model=embed_size, nhead=num_heads, num_encoder_layers=num_layers, num_decoder_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 1000, embed_size))\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        features = self.feature_embed(features)\n",
    "        features = features.unsqueeze(1)\n",
    "        captions = self.embed(captions)\n",
    "        captions = captions + self.positional_encoding[:, :captions.size(1), :]\n",
    "        \n",
    "        transformer_out = self.transformer(features.permute(1, 0, 2), captions.permute(1, 0, 2))\n",
    "        output = self.fc_out(transformer_out)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = 10000  # 예시 값, 실제로는 데이터셋에 맞게 설정해야 함\n",
    "embed_size = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "feature_dim = 4096  # VGG16의 출력 차원\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = ImageCaptioningModel(feature_dim, vocab_size, embed_size, num_heads, num_layers)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 루프\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, captions in dataloader:\n",
    "        features = extract_features(images)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features, captions[:, :-1])\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
